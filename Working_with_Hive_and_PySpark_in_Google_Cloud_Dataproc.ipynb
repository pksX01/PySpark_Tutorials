{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Working with Hive and PySpark in Google Cloud Dataproc.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pksX01/PySpark_Tutorials/blob/main/Working_with_Hive_and_PySpark_in_Google_Cloud_Dataproc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52ad661e-55e3-451e-b7c5-7ecb807da305",
        "outputId": "1fa4bff3-677d-45dd-f417-a14ce703019e"
      },
      "source": [
        "sc"
      ],
      "id": "52ad661e-55e3-451e-b7c5-7ecb807da305",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://big-data-cluster-m.us-central1-f.c.deeplearning-248116.internal:45809\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>yarn</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySparkShell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=yarn appName=PySparkShell>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b4bfeb6-6dd0-4765-abb2-c75541f02e84",
        "outputId": "814cb0cf-f3d8-4384-cb11-88194ad622db"
      },
      "source": [
        "spark"
      ],
      "id": "9b4bfeb6-6dd0-4765-abb2-c75541f02e84",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://big-data-cluster-m.us-central1-f.c.deeplearning-248116.internal:45809\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>yarn</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySparkShell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd123643280>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ded227f-b1f4-4fe3-b46d-789c6a75c13c"
      },
      "source": [
        "df = spark.read.option('header', 'true').csv('gs://datsets-for-big-data/stroke_data/healthcare-dataset-stroke-data.csv')"
      ],
      "id": "7ded227f-b1f4-4fe3-b46d-789c6a75c13c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7f486cc-de63-445b-87f1-b6555420b9f5",
        "outputId": "4d09c03f-5ad5-4d97-c90f-82152989d027"
      },
      "source": [
        "df.show(5)"
      ],
      "id": "e7f486cc-de63-445b-87f1-b6555420b9f5",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------+---+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
            "|   id|gender|age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level| bmi| smoking_status|stroke|\n",
            "+-----+------+---+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
            "| 9046|  Male| 67|           0|            1|         Yes|      Private|         Urban|           228.69|36.6|formerly smoked|     1|\n",
            "|51676|Female| 61|           0|            0|         Yes|Self-employed|         Rural|           202.21| N/A|   never smoked|     1|\n",
            "|31112|  Male| 80|           0|            1|         Yes|      Private|         Rural|           105.92|32.5|   never smoked|     1|\n",
            "|60182|Female| 49|           0|            0|         Yes|      Private|         Urban|           171.23|34.4|         smokes|     1|\n",
            "| 1665|Female| 79|           1|            0|         Yes|Self-employed|         Rural|           174.12|  24|   never smoked|     1|\n",
            "+-----+------+---+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9839747d-9d27-4938-bf88-e28c3aa80d4d"
      },
      "source": [
        "new_df = df.select('id', 'gender', 'age', 'stroke')"
      ],
      "id": "9839747d-9d27-4938-bf88-e28c3aa80d4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "010a681f-21cf-4b7c-95d9-25101e2e25db",
        "outputId": "d038bf8f-6097-476f-b89e-b975156c24fe"
      },
      "source": [
        "new_df.write.option('header', 'true').csv('/user/spark/sample_stroke_data')"
      ],
      "id": "010a681f-21cf-4b7c-95d9-25101e2e25db",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e850a45f-a905-4e17-aa58-383df2ea0d94"
      },
      "source": [
        "**Working with Hive Tables**"
      ],
      "id": "e850a45f-a905-4e17-aa58-383df2ea0d94"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00343ae8-936d-446b-b3b5-768a2f51bae1",
        "outputId": "a23dd858-e74a-4058-cddf-5763f1006a2f"
      },
      "source": [
        "spark.sql(\"show databases\").show()"
      ],
      "id": "00343ae8-936d-446b-b3b5-768a2f51bae1",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "| namespace|\n",
            "+----------+\n",
            "|   default|\n",
            "|   finance|\n",
            "|healthcare|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6f167e1-b5cc-432f-be87-7eaac6b09c05"
      },
      "source": [
        "stocks_df = spark.sql(\"select * from finance.stocks\")"
      ],
      "id": "c6f167e1-b5cc-432f-be87-7eaac6b09c05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0320cd60-4ca3-46bd-b8ab-8392b8c0464d",
        "outputId": "fa398ee1-6837-4e92-bbd5-8059e6df3205"
      },
      "source": [
        "stocks_df.show(5)"
      ],
      "id": "0320cd60-4ca3-46bd-b8ab-8392b8c0464d",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------+------+------+------+-------+-------+\n",
            "|trading_date|  open|  high|   low| close| volume|openint|\n",
            "+------------+------+------+------+------+-------+-------+\n",
            "|        null|  null|  null|  null|  null|   null|   null|\n",
            "|  2010-07-21|24.333|24.333|23.946|23.946|43321.0|      0|\n",
            "|  2010-07-22|24.644|24.644|24.362|24.487|18031.0|      0|\n",
            "|  2010-07-23|24.759|24.759|24.314|24.507| 8897.0|      0|\n",
            "|  2010-07-26|24.624|24.624|24.449|24.595|19443.0|      0|\n",
            "+------------+------+------+------+------+-------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f973d7c-9b24-49e6-924e-7f4639607344"
      },
      "source": [
        "import pyspark.sql.functions as f\n",
        "stocks_avg_df = stocks_df.dropna().withColumn('year', f.year(f.to_date('trading_date', 'yyyy-MM-dd'))).groupBy('year')\\\n",
        "        .agg(\n",
        "            f.avg('open').alias('average_open'),\\\n",
        "            f.avg('close').alias('average_close'),\\\n",
        "            f.avg('low').alias('average_low'),\\\n",
        "            f.avg('high').alias('average_high'),\\\n",
        "        )"
      ],
      "id": "0f973d7c-9b24-49e6-924e-7f4639607344",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cae39d2-cfd6-4e8a-a330-78202d0e8cd7",
        "outputId": "e212a3fc-2b7d-4419-bd21-3096dee2de17"
      },
      "source": [
        "stocks_avg_df.show()"
      ],
      "id": "1cae39d2-cfd6-4e8a-a330-78202d0e8cd7",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------------------+----------------+------------------+------------------+\n",
            "|year|      average_open|   average_close|       average_low|      average_high|\n",
            "+----+------------------+----------------+------------------+------------------+\n",
            "|2010|26.865070866272514|26.8716991728386| 26.77238933596991|26.936309814453125|\n",
            "|2011|29.679443492060123|29.6788086766782|29.605217394621477|29.751608740765114|\n",
            "+----+------------------+----------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f63e8272-6af9-40f3-84c2-ff777991c7b9",
        "outputId": "dca57cf4-7b22-4e98-a790-994a4086d387"
      },
      "source": [
        "%%writefile stocks_transformation.py\n",
        "\n",
        "import sys\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "db = sys.argv[1]\n",
        "tbl = sys.argv[2]\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Transformations on Stocks data\").enableHiveSupport().getOrCreate()\n",
        "\n",
        "stocks_df = spark.sql(\"select * from {}.{}\".format(db, tbl))\n",
        "\n",
        "transformed_stocks_df = stocks_df.dropna().withColumn('year', f.year(f.to_date('trading_date', 'yyyy-MM-dd'))).groupBy('year')\\\n",
        "        .agg(\n",
        "            f.avg('open').alias('average_open'),\\\n",
        "            f.avg('close').alias('average_close'),\\\n",
        "            f.avg('low').alias('average_low'),\\\n",
        "            f.avg('high').alias('average_high'),\\\n",
        "        )\n",
        "\n",
        "transformed_stocks_df.write.mode(\"overwrite\").saveAsTable(\"finance.transformed_stocks\")"
      ],
      "id": "f63e8272-6af9-40f3-84c2-ff777991c7b9",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting stocks_transformation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dfe4989-4831-42b8-856c-0aeef89c0e95"
      },
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "    # The ID of your GCS bucket\n",
        "    # bucket_name = \"your-bucket-name\"\n",
        "    # The path to your file to upload\n",
        "    # source_file_name = \"local/path/to/file\"\n",
        "    # The ID of your GCS object\n",
        "    # destination_blob_name = \"storage-object-name\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(\n",
        "        \"File {} uploaded to {}.\".format(\n",
        "            source_file_name, destination_blob_name\n",
        "        )\n",
        "    )\n"
      ],
      "id": "5dfe4989-4831-42b8-856c-0aeef89c0e95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72b4336a-bd72-4508-8f62-4fb33de1884c",
        "outputId": "e7d32564-5eb3-401b-ba2a-87bb5d13708d"
      },
      "source": [
        "upload_blob('datsets-for-big-data', 'stocks_transformation.py', 'python_files/stocks_transformation.py')"
      ],
      "id": "72b4336a-bd72-4508-8f62-4fb33de1884c",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File stocks_transformation.py uploaded to python_files/stocks_transformation.py.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d500b4d-4fdc-4a5e-a62e-172b93fd7f94"
      },
      "source": [
        ""
      ],
      "id": "8d500b4d-4fdc-4a5e-a62e-172b93fd7f94",
      "execution_count": null,
      "outputs": []
    }
  ]
}